{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# NCAAM Network-Based Team Strength Analysis\n",
    "\n",
    "## Objective\n",
    "Implement network-based strength propagation for team tier classification:\n",
    "- **Iterative Strength Ratings**: Massey/PageRank-style algorithm\n",
    "- **Network-Weighted SOS**: Strength of schedule using network ratings\n",
    "- **Multi-Hop Common Opponents**: 3-level transitive relationships\n",
    "\n",
    "## NCAAM-Specific Characteristics\n",
    "- 350+ teams, ~30-game season per team\n",
    "- **Sparse graph** with **conference clusters**\n",
    "- Cross-conference games create bridges between clusters\n",
    "- Need deeper path exploration to connect distant teams\n",
    "- Parameters: `max_hops=3`, `recency_decay=0.85`, `margin_cap=20`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "from collections import Counter\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# NCAAM-specific config\n",
    "CONFIG = {\n",
    "    'sport': 'NCAAM',\n",
    "    'max_hops': 3,          # Need deeper paths across conferences\n",
    "    'recency_decay': 0.85,  # Early season less relevant\n",
    "    'margin_cap': 20,       # Similar to NFL\n",
    "    'iterations': 150,      # More iterations for larger network\n",
    "    'tolerance': 0.0005     # Tighter tolerance for convergence\n",
    "}\n",
    "\n",
    "print(f\"Config: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## Phase 1: Data Loading & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NCAAM data\n",
    "data_file = Path().resolve().parent / 'data' / 'results' / 'ncaam_season_results.xlsx'\n",
    "df = pd.read_excel(data_file)\n",
    "\n",
    "print(f\"Loaded {len(df)} NCAAM games\")\n",
    "print(f\"Date range: {df['game_date'].min().date()} to {df['game_date'].max().date()}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prep-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare game-level data\n",
    "df['home_margin'] = df['home_score'] - df['away_score']\n",
    "df['winner'] = np.where(df['home_margin'] > 0, df['home_team'], \n",
    "                        np.where(df['home_margin'] < 0, df['away_team'], None))\n",
    "df['loser'] = np.where(df['home_margin'] > 0, df['away_team'], \n",
    "                       np.where(df['home_margin'] < 0, df['home_team'], None))\n",
    "df['margin'] = df['home_margin'].abs()\n",
    "\n",
    "games_with_result = df[df['winner'].notna()].copy()\n",
    "print(f\"Games with decisive result: {len(games_with_result)}\")\n",
    "\n",
    "# Get all teams\n",
    "all_teams = set(df['home_team'].unique()) | set(df['away_team'].unique())\n",
    "print(f\"Total teams: {len(all_teams)}\")\n",
    "\n",
    "# Avg games per team\n",
    "games_per_team = (len(df) * 2) / len(all_teams)\n",
    "print(f\"Avg games per team: {games_per_team:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-header",
   "metadata": {},
   "source": [
    "## Phase 2: Build Network Graph\n",
    "\n",
    "NCAAM has a sparse graph with conference clusters. We'll analyze connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_team_network(games_df, recency_decay=0.85):\n",
    "    \"\"\"Build weighted directed graph - optimized for sparse NCAAM network.\"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    for team in all_teams:\n",
    "        G.add_node(team)\n",
    "    \n",
    "    games_sorted = games_df.sort_values('game_date')\n",
    "    max_date = games_sorted['game_date'].max()\n",
    "    \n",
    "    edge_data = {}\n",
    "    \n",
    "    for _, game in games_sorted.iterrows():\n",
    "        winner, loser = game['winner'], game['loser']\n",
    "        margin = game['margin']\n",
    "        \n",
    "        days_ago = (max_date - game['game_date']).days\n",
    "        recency_weight = recency_decay ** (days_ago / 7)\n",
    "        \n",
    "        key = (winner, loser)\n",
    "        if key not in edge_data:\n",
    "            edge_data[key] = {\n",
    "                'games': 0,\n",
    "                'total_margin': 0,\n",
    "                'weighted_margin': 0,\n",
    "                'total_weight': 0\n",
    "            }\n",
    "        \n",
    "        edge_data[key]['games'] += 1\n",
    "        edge_data[key]['total_margin'] += margin\n",
    "        edge_data[key]['weighted_margin'] += margin * recency_weight\n",
    "        edge_data[key]['total_weight'] += recency_weight\n",
    "    \n",
    "    for (winner, loser), data in edge_data.items():\n",
    "        avg_margin = data['total_margin'] / data['games']\n",
    "        weighted_avg = data['weighted_margin'] / data['total_weight']\n",
    "        \n",
    "        G.add_edge(winner, loser, \n",
    "                   games=data['games'],\n",
    "                   avg_margin=avg_margin,\n",
    "                   weighted_margin=weighted_avg)\n",
    "    \n",
    "    return G\n",
    "\n",
    "G = build_team_network(games_with_result, CONFIG['recency_decay'])\n",
    "\n",
    "print(f\"Network Summary:\")\n",
    "print(f\"  Nodes (teams): {G.number_of_nodes()}\")\n",
    "print(f\"  Edges: {G.number_of_edges()}\")\n",
    "print(f\"  Avg degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.1f}\")\n",
    "print(f\"  Density: {nx.density(G):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyze-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze graph connectivity - critical for NCAAM\n",
    "G_undirected = G.to_undirected()\n",
    "\n",
    "# Find connected components\n",
    "components = list(nx.connected_components(G_undirected))\n",
    "print(f\"\\nConnectivity Analysis:\")\n",
    "print(f\"  Connected components: {len(components)}\")\n",
    "print(f\"  Largest component: {len(max(components, key=len))} teams\")\n",
    "\n",
    "if len(components) > 1:\n",
    "    print(f\"  Isolated teams: {sum(1 for c in components if len(c) == 1)}\")\n",
    "    small_components = [c for c in components if 1 < len(c) < 10]\n",
    "    print(f\"  Small clusters (2-9 teams): {len(small_components)}\")\n",
    "\n",
    "# Degree distribution\n",
    "degrees = [d for _, d in G.degree()]\n",
    "print(f\"\\nDegree Distribution:\")\n",
    "print(f\"  Min: {min(degrees)}, Max: {max(degrees)}, Median: {np.median(degrees):.0f}\")\n",
    "\n",
    "# Low-connectivity teams (potential issues)\n",
    "low_conn_teams = [n for n, d in G.degree() if d < 5]\n",
    "print(f\"  Teams with <5 connections: {len(low_conn_teams)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detect-clusters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect conference-like clusters using community detection\n",
    "try:\n",
    "    from networkx.algorithms.community import louvain_communities\n",
    "    communities = louvain_communities(G_undirected, seed=42)\n",
    "    \n",
    "    print(f\"\\nCommunity Detection (Conference Proxy):\")\n",
    "    print(f\"  Communities found: {len(communities)}\")\n",
    "    \n",
    "    # Size distribution\n",
    "    sizes = sorted([len(c) for c in communities], reverse=True)\n",
    "    print(f\"  Largest 10 communities: {sizes[:10]}\")\n",
    "    \n",
    "    # Store community membership\n",
    "    team_community = {}\n",
    "    for i, comm in enumerate(communities):\n",
    "        for team in comm:\n",
    "            team_community[team] = i\n",
    "            \n",
    "except ImportError:\n",
    "    print(\"Community detection not available (needs networkx >= 2.6)\")\n",
    "    team_community = {team: 0 for team in all_teams}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-graph",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize (sample of top teams only due to size)\n# Calculate simple win% for filtering\nwin_counts = dict(G.out_degree())\nloss_counts = dict(G.in_degree())\nwin_pct = {team: win_counts.get(team, 0) / (win_counts.get(team, 0) + loss_counts.get(team, 0) + 0.001)\n           for team in all_teams}\n\n# Filter to top 50 teams by win%\ntop_teams = sorted(win_pct.items(), key=lambda x: -x[1])[:50]\ntop_team_names = [t[0] for t in top_teams]\n\nG_sub = G.subgraph(top_team_names).copy()\n\nfig, ax = plt.subplots(figsize=(14, 14))\n\nnode_colors = [win_pct.get(node, 0.5) for node in G_sub.nodes()]\npos = nx.spring_layout(G_sub, k=2, iterations=50, seed=42)\n\nnodes = nx.draw_networkx_nodes(G_sub, pos, node_color=node_colors, cmap=plt.cm.RdYlGn,\n                               node_size=400, alpha=0.8, vmin=0.3, vmax=0.9, ax=ax)\nnx.draw_networkx_labels(G_sub, pos, font_size=6, ax=ax)\nnx.draw_networkx_edges(G_sub, pos, alpha=0.15, arrows=True, arrowsize=8, ax=ax)\n\nax.set_title(f'NCAAM Top 50 Teams Network\\n({G.number_of_nodes()} total teams)', fontsize=14)\nfig.colorbar(nodes, ax=ax, label='Win %')\nax.axis('off')\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "strength-header",
   "metadata": {},
   "source": [
    "## Phase 3: Iterative Strength Rating\n",
    "\n",
    "For NCAAM's sparse graph, we need more iterations and careful handling of isolated teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strength-algo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iterative_strength(games_df, max_iterations=150, tolerance=0.0005, margin_cap=20):\n",
    "    \"\"\"\n",
    "    Compute network strength ratings - optimized for sparse NCAAM network.\n",
    "    \n",
    "    Uses smaller learning rate and more iterations for large sparse graph.\n",
    "    \"\"\"\n",
    "    teams = set(games_df['home_team']) | set(games_df['away_team'])\n",
    "    ratings = {team: 0.5 for team in teams}\n",
    "    games = games_df[games_df['winner'].notna()].copy()\n",
    "    \n",
    "    # Smaller learning rate for stability with many teams\n",
    "    learning_rate = 0.03\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        new_ratings = ratings.copy()\n",
    "        \n",
    "        for _, game in games.iterrows():\n",
    "            winner = game['winner']\n",
    "            loser = game['loser']\n",
    "            margin = min(game['margin'], margin_cap)\n",
    "            \n",
    "            winner_rating = ratings[winner]\n",
    "            loser_rating = ratings[loser]\n",
    "            \n",
    "            total = winner_rating + loser_rating\n",
    "            expected = winner_rating / total if total > 0 else 0.5\n",
    "            \n",
    "            surprise = 1 - expected\n",
    "            adjustment = surprise * (margin / margin_cap) * learning_rate\n",
    "            \n",
    "            new_ratings[winner] += adjustment\n",
    "            new_ratings[loser] -= adjustment\n",
    "        \n",
    "        # Normalize\n",
    "        min_r = min(new_ratings.values())\n",
    "        max_r = max(new_ratings.values())\n",
    "        if max_r > min_r:\n",
    "            new_ratings = {t: (r - min_r) / (max_r - min_r) for t, r in new_ratings.items()}\n",
    "        \n",
    "        max_change = max(abs(new_ratings[t] - ratings[t]) for t in teams)\n",
    "        history.append(max_change)\n",
    "        \n",
    "        if max_change < tolerance:\n",
    "            print(f\"Converged at iteration {iteration + 1}\")\n",
    "            break\n",
    "        \n",
    "        ratings = new_ratings\n",
    "    else:\n",
    "        print(f\"Did not converge in {max_iterations} iterations (final change: {max_change:.6f})\")\n",
    "    \n",
    "    return ratings, history\n",
    "\n",
    "# Compute ratings\n",
    "network_ratings, convergence_history = compute_iterative_strength(\n",
    "    df, \n",
    "    max_iterations=CONFIG['iterations'],\n",
    "    tolerance=CONFIG['tolerance'],\n",
    "    margin_cap=CONFIG['margin_cap']\n",
    ")\n",
    "\n",
    "# Top teams\n",
    "ratings_df = pd.DataFrame([\n",
    "    {'team': team, 'network_rating': rating}\n",
    "    for team, rating in network_ratings.items()\n",
    "]).sort_values('network_rating', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Teams by Network Rating:\")\n",
    "print(ratings_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(convergence_history)\n",
    "plt.axhline(y=CONFIG['tolerance'], color='r', linestyle='--', label=f\"Tolerance ({CONFIG['tolerance']})\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Max Rating Change')\n",
    "plt.title('NCAAM: Convergence of Iterative Strength Algorithm')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-header",
   "metadata": {},
   "source": [
    "## Phase 4: Validate Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "calc-simple-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate simple team stats\n",
    "team_stats = []\n",
    "for team in all_teams:\n",
    "    home_games = df[df['home_team'] == team]\n",
    "    away_games = df[df['away_team'] == team]\n",
    "    \n",
    "    home_wins = (home_games['home_score'] > home_games['away_score']).sum()\n",
    "    away_wins = (away_games['away_score'] > away_games['home_score']).sum()\n",
    "    \n",
    "    home_diff = (home_games['home_score'] - home_games['away_score']).sum()\n",
    "    away_diff = (away_games['away_score'] - away_games['home_score']).sum()\n",
    "    \n",
    "    total_games = len(home_games) + len(away_games)\n",
    "    \n",
    "    opponents = list(home_games['away_team']) + list(away_games['home_team'])\n",
    "    \n",
    "    team_stats.append({\n",
    "        'team': team,\n",
    "        'games': total_games,\n",
    "        'wins': home_wins + away_wins,\n",
    "        'simple_win_pct': (home_wins + away_wins) / total_games if total_games > 0 else 0,\n",
    "        'point_diff_avg': (home_diff + away_diff) / total_games if total_games > 0 else 0,\n",
    "        'network_rating': network_ratings.get(team, 0.5),\n",
    "        'community': team_community.get(team, -1),\n",
    "        'opponents': opponents\n",
    "    })\n",
    "\n",
    "df_teams = pd.DataFrame(team_stats)\n",
    "\n",
    "# Calculate SOS\n",
    "win_pct_map = df_teams.set_index('team')['simple_win_pct'].to_dict()\n",
    "df_teams['simple_sos'] = df_teams['opponents'].apply(\n",
    "    lambda opps: np.mean([win_pct_map.get(o, 0.5) for o in opps]) if opps else 0.5\n",
    ")\n",
    "df_teams['network_sos'] = df_teams['opponents'].apply(\n",
    "    lambda opps: np.mean([network_ratings.get(o, 0.5) for o in opps]) if opps else 0.5\n",
    ")\n",
    "\n",
    "df_teams = df_teams.drop('opponents', axis=1).sort_values('network_rating', ascending=False)\n",
    "print(f\"Team stats calculated for {len(df_teams)} teams\")\n",
    "df_teams.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-ratings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "# Filter to teams with enough games for reliable stats\n",
    "df_reliable = df_teams[df_teams['games'] >= 15].copy()\n",
    "print(f\"Teams with 15+ games: {len(df_reliable)}\")\n",
    "\n",
    "corr_win_pct = df_reliable['network_rating'].corr(df_reliable['simple_win_pct'])\n",
    "corr_point_diff = df_reliable['network_rating'].corr(df_reliable['point_diff_avg'])\n",
    "corr_sos = df_reliable['network_sos'].corr(df_reliable['simple_sos'])\n",
    "\n",
    "print(f\"\\nCorrelation Analysis (teams with 15+ games):\")\n",
    "print(f\"  Network Rating vs Win%: {corr_win_pct:.3f}\")\n",
    "print(f\"  Network Rating vs Point Diff: {corr_point_diff:.3f}\")\n",
    "print(f\"  Network SOS vs Simple SOS: {corr_sos:.3f}\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].scatter(df_reliable['simple_win_pct'], df_reliable['network_rating'], alpha=0.5, s=20)\n",
    "axes[0].set_xlabel('Simple Win %')\n",
    "axes[0].set_ylabel('Network Rating')\n",
    "axes[0].set_title(f'Network vs Win % (r={corr_win_pct:.3f})')\n",
    "\n",
    "axes[1].scatter(df_reliable['point_diff_avg'], df_reliable['network_rating'], alpha=0.5, s=20)\n",
    "axes[1].set_xlabel('Avg Point Differential')\n",
    "axes[1].set_ylabel('Network Rating')\n",
    "axes[1].set_title(f'Network vs Point Diff (r={corr_point_diff:.3f})')\n",
    "\n",
    "axes[2].scatter(df_reliable['simple_sos'], df_reliable['network_sos'], alpha=0.5, s=20)\n",
    "axes[2].set_xlabel('Simple SOS')\n",
    "axes[2].set_ylabel('Network SOS')\n",
    "axes[2].set_title(f'Network SOS vs Simple SOS (r={corr_sos:.3f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tier-header",
   "metadata": {},
   "source": [
    "## Phase 5: Network-Based Tier Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classify-tiers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify tiers (using reliable teams only)\n",
    "q75_simple, q25_simple = df_reliable['simple_win_pct'].quantile([0.75, 0.25])\n",
    "q75_network, q25_network = df_reliable['network_rating'].quantile([0.75, 0.25])\n",
    "\n",
    "df_teams['simple_tier'] = df_teams['simple_win_pct'].apply(\n",
    "    lambda x: 'Elite' if x >= q75_simple else ('Bottom' if x <= q25_simple else 'Mid')\n",
    ")\n",
    "df_teams['network_tier'] = df_teams['network_rating'].apply(\n",
    "    lambda x: 'Elite' if x >= q75_network else ('Bottom' if x <= q25_network else 'Mid')\n",
    ")\n",
    "\n",
    "print(\"Tier Classification:\")\n",
    "print(f\"  Simple thresholds: Elite >= {q75_simple:.3f}, Bottom <= {q25_simple:.3f}\")\n",
    "print(f\"  Network thresholds: Elite >= {q75_network:.3f}, Bottom <= {q25_network:.3f}\")\n",
    "\n",
    "# Agreement\n",
    "df_compare = df_teams[df_teams['games'] >= 15]\n",
    "agreement = pd.crosstab(df_compare['simple_tier'], df_compare['network_tier'], margins=True)\n",
    "print(f\"\\nTier Agreement (teams with 15+ games):\")\n",
    "print(agreement)\n",
    "\n",
    "# Significant disagreements\n",
    "disagreements = df_compare[\n",
    "    (df_compare['simple_tier'] == 'Elite') & (df_compare['network_tier'] != 'Elite') |\n",
    "    (df_compare['simple_tier'] != 'Elite') & (df_compare['network_tier'] == 'Elite')\n",
    "]\n",
    "print(f\"\\nElite tier disagreements ({len(disagreements)}):\")\n",
    "print(disagreements[['team', 'games', 'simple_win_pct', 'network_rating', \n",
    "                     'simple_sos', 'network_sos', 'simple_tier', 'network_tier']].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-opp-header",
   "metadata": {},
   "source": [
    "## Phase 6: Multi-Hop Common Opponent Analysis\n",
    "\n",
    "For NCAAM, multi-hop paths are critical for connecting teams across conferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-hop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_common_opponent_paths(G, team_a, team_b, max_hops=3):\n",
    "    \"\"\"Find paths through common opponents - optimized for sparse graphs.\"\"\"\n",
    "    G_undirected = G.to_undirected()\n",
    "    \n",
    "    if team_a not in G or team_b not in G:\n",
    "        return []\n",
    "    \n",
    "    # Check if teams are in same component\n",
    "    if not nx.has_path(G_undirected, team_a, team_b):\n",
    "        return []\n",
    "    \n",
    "    paths = []\n",
    "    try:\n",
    "        # Limit paths to avoid combinatorial explosion\n",
    "        path_gen = nx.all_simple_paths(G_undirected, team_a, team_b, cutoff=max_hops + 1)\n",
    "        for i, path in enumerate(path_gen):\n",
    "            if len(path) > 2:\n",
    "                paths.append(path)\n",
    "            if i > 1000:  # Limit for performance\n",
    "                break\n",
    "    except nx.NetworkXNoPath:\n",
    "        pass\n",
    "    \n",
    "    return paths\n",
    "\n",
    "def evaluate_matchup_via_paths(G, team_a, team_b, ratings, max_hops=3):\n",
    "    \"\"\"Evaluate teams through common opponent network.\"\"\"\n",
    "    paths = find_common_opponent_paths(G, team_a, team_b, max_hops)\n",
    "    \n",
    "    if not paths:\n",
    "        return None, 0, []\n",
    "    \n",
    "    a_total_score = 0\n",
    "    b_total_score = 0\n",
    "    total_weight = 0\n",
    "    path_details = []\n",
    "    \n",
    "    for path in paths:\n",
    "        intermediates = path[1:-1]\n",
    "        a_path_score = 0\n",
    "        b_path_score = 0\n",
    "        path_weight = 0\n",
    "        \n",
    "        for intermediate in intermediates:\n",
    "            int_rating = ratings.get(intermediate, 0.5)\n",
    "            \n",
    "            if G.has_edge(team_a, intermediate):\n",
    "                margin = G[team_a][intermediate].get('avg_margin', 0)\n",
    "                a_path_score += margin * int_rating\n",
    "            elif G.has_edge(intermediate, team_a):\n",
    "                margin = G[intermediate][team_a].get('avg_margin', 0)\n",
    "                a_path_score -= margin * int_rating\n",
    "            \n",
    "            if G.has_edge(team_b, intermediate):\n",
    "                margin = G[team_b][intermediate].get('avg_margin', 0)\n",
    "                b_path_score += margin * int_rating\n",
    "            elif G.has_edge(intermediate, team_b):\n",
    "                margin = G[intermediate][team_b].get('avg_margin', 0)\n",
    "                b_path_score -= margin * int_rating\n",
    "            \n",
    "            path_weight += int_rating\n",
    "        \n",
    "        if path_weight > 0:\n",
    "            a_total_score += a_path_score\n",
    "            b_total_score += b_path_score\n",
    "            total_weight += path_weight\n",
    "            \n",
    "            path_details.append({\n",
    "                'path': ' -> '.join(path),\n",
    "                'hops': len(path) - 2,\n",
    "                'a_score': a_path_score / path_weight if path_weight else 0,\n",
    "                'b_score': b_path_score / path_weight if path_weight else 0\n",
    "            })\n",
    "    \n",
    "    if total_weight == 0:\n",
    "        return None, 0, []\n",
    "    \n",
    "    edge = (a_total_score - b_total_score) / total_weight\n",
    "    return edge, len(paths), path_details\n",
    "\n",
    "# Example: Compare top team vs a mid-tier team\n",
    "top_team = df_teams.iloc[0]['team']\n",
    "mid_team = df_teams[df_teams['network_tier'] == 'Mid'].iloc[0]['team']\n",
    "\n",
    "edge, num_paths, details = evaluate_matchup_via_paths(G, top_team, mid_team, network_ratings, CONFIG['max_hops'])\n",
    "\n",
    "print(f\"Matchup Analysis: {top_team} vs {mid_team}\")\n",
    "print(f\"Paths found: {num_paths}\")\n",
    "if edge is not None:\n",
    "    print(f\"Common opponent edge: {edge:.2f} ({'favors ' + top_team if edge > 0 else 'favors ' + mid_team})\")\n",
    "    \n",
    "    if details:\n",
    "        path_df = pd.DataFrame(details)\n",
    "        print(f\"\\nPaths by hop count:\")\n",
    "        print(path_df.groupby('hops').size())\n",
    "else:\n",
    "    print(\"No paths found - teams may be in different components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cross-conference connectivity\n",
    "# Find paths between teams in different communities\n",
    "\n",
    "def analyze_cross_conference_paths(G, team_community, ratings, sample_size=20):\n",
    "    \"\"\"Analyze path lengths between teams in different communities.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Sample teams from different communities\n",
    "    comm_teams = {}\n",
    "    for team, comm in team_community.items():\n",
    "        if comm not in comm_teams:\n",
    "            comm_teams[comm] = []\n",
    "        comm_teams[comm].append(team)\n",
    "    \n",
    "    # Get top teams from different large communities\n",
    "    large_comms = [c for c, teams in comm_teams.items() if len(teams) >= 10][:5]\n",
    "    \n",
    "    if len(large_comms) < 2:\n",
    "        print(\"Not enough large communities for cross-conference analysis\")\n",
    "        return None\n",
    "    \n",
    "    G_und = G.to_undirected()\n",
    "    \n",
    "    for i, comm1 in enumerate(large_comms):\n",
    "        for comm2 in large_comms[i+1:]:\n",
    "            # Sample teams from each community\n",
    "            teams1 = comm_teams[comm1][:3]\n",
    "            teams2 = comm_teams[comm2][:3]\n",
    "            \n",
    "            for t1 in teams1:\n",
    "                for t2 in teams2:\n",
    "                    if t1 in G and t2 in G:\n",
    "                        try:\n",
    "                            path_len = nx.shortest_path_length(G_und, t1, t2)\n",
    "                            results.append({\n",
    "                                'comm1': comm1,\n",
    "                                'comm2': comm2,\n",
    "                                'team1': t1,\n",
    "                                'team2': t2,\n",
    "                                'path_length': path_len\n",
    "                            })\n",
    "                        except nx.NetworkXNoPath:\n",
    "                            results.append({\n",
    "                                'comm1': comm1,\n",
    "                                'comm2': comm2,\n",
    "                                'team1': t1,\n",
    "                                'team2': t2,\n",
    "                                'path_length': -1  # No path\n",
    "                            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "cross_conf_df = analyze_cross_conference_paths(G, team_community, network_ratings)\n",
    "if cross_conf_df is not None and len(cross_conf_df) > 0:\n",
    "    valid_paths = cross_conf_df[cross_conf_df['path_length'] > 0]\n",
    "    print(f\"\\nCross-Conference Path Analysis:\")\n",
    "    print(f\"  Pairs analyzed: {len(cross_conf_df)}\")\n",
    "    print(f\"  Pairs with paths: {len(valid_paths)}\")\n",
    "    if len(valid_paths) > 0:\n",
    "        print(f\"  Avg path length: {valid_paths['path_length'].mean():.1f}\")\n",
    "        print(f\"  Path length distribution:\")\n",
    "        print(valid_paths['path_length'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coverage-header",
   "metadata": {},
   "source": [
    "## Phase 7: Coverage Analysis by Network Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coverage-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate handicap coverage\n",
    "handicaps = [0, 5, 10, 11, 13, 15]\n",
    "\n",
    "for h in handicaps:\n",
    "    df[f'home_covers_{h}pt'] = (df['spread_result_difference'] + h) >= 0\n",
    "    df[f'away_covers_{h}pt'] = df['spread_result_difference'] <= h\n",
    "\n",
    "# Analyze coverage (use teams with 15+ games)\n",
    "reliable_teams = df_teams[df_teams['games'] >= 15]['team'].tolist()\n",
    "results = []\n",
    "\n",
    "for h in handicaps:\n",
    "    for tier_type in ['simple_tier', 'network_tier']:\n",
    "        for tier in ['Elite', 'Mid', 'Bottom']:\n",
    "            tier_teams = df_teams[(df_teams[tier_type] == tier) & \n",
    "                                  (df_teams['team'].isin(reliable_teams))]['team'].tolist()\n",
    "            \n",
    "            total_games = 0\n",
    "            total_covers = 0\n",
    "            \n",
    "            for team in tier_teams:\n",
    "                hg = df[df['home_team'] == team]\n",
    "                total_games += len(hg)\n",
    "                total_covers += hg[f'home_covers_{h}pt'].sum()\n",
    "                \n",
    "                ag = df[df['away_team'] == team]\n",
    "                total_games += len(ag)\n",
    "                total_covers += ag[f'away_covers_{h}pt'].sum()\n",
    "            \n",
    "            results.append({\n",
    "                'handicap': h,\n",
    "                'tier_type': tier_type,\n",
    "                'tier': tier,\n",
    "                'games': total_games,\n",
    "                'covers': total_covers,\n",
    "                'cover_pct': total_covers / total_games if total_games > 0 else 0\n",
    "            })\n",
    "\n",
    "df_coverage = pd.DataFrame(results)\n",
    "\n",
    "print(\"Coverage % by Tier Type and Handicap:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for tier_type in ['simple_tier', 'network_tier']:\n",
    "    print(f\"\\n{tier_type.replace('_', ' ').title()}:\")\n",
    "    pivot = df_coverage[df_coverage['tier_type'] == tier_type].pivot(\n",
    "        index='handicap', columns='tier', values='cover_pct'\n",
    "    )[['Elite', 'Mid', 'Bottom']]\n",
    "    print((pivot * 100).round(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coverage comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = {'Elite': '#2ecc71', 'Mid': '#f39c12', 'Bottom': '#e74c3c'}\n",
    "\n",
    "for idx, tier_type in enumerate(['simple_tier', 'network_tier']):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for tier in ['Elite', 'Mid', 'Bottom']:\n",
    "        data = df_coverage[(df_coverage['tier_type'] == tier_type) & (df_coverage['tier'] == tier)]\n",
    "        ax.plot(data['handicap'], data['cover_pct'] * 100, \n",
    "                marker='o', label=tier, color=colors[tier], linewidth=2)\n",
    "    \n",
    "    ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.set_xlabel('Handicap Points')\n",
    "    ax.set_ylabel('Coverage %')\n",
    "    ax.set_title(f'{tier_type.replace(\"_\", \" \").title()}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('NCAAM: Coverage by Tier Classification Method', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"NCAAM NETWORK STRENGTH ANALYSIS: SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. DATA\")\n",
    "print(f\"   - Games analyzed: {len(df)}\")\n",
    "print(f\"   - Teams: {len(all_teams)}\")\n",
    "print(f\"   - Network edges: {G.number_of_edges()}\")\n",
    "print(f\"   - Graph density: {nx.density(G):.4f}\")\n",
    "print(f\"   - Connected components: {len(components)}\")\n",
    "\n",
    "print(f\"\\n2. NETWORK RATING VALIDATION (teams with 15+ games)\")\n",
    "print(f\"   - Correlation with Win%: {corr_win_pct:.3f}\")\n",
    "print(f\"   - Correlation with Point Diff: {corr_point_diff:.3f}\")\n",
    "print(f\"   - Network SOS vs Simple SOS: {corr_sos:.3f}\")\n",
    "\n",
    "print(f\"\\n3. TIER CLASSIFICATION\")\n",
    "tier_agreement = (df_compare['simple_tier'] == df_compare['network_tier']).mean() * 100\n",
    "print(f\"   - Agreement rate: {tier_agreement:.1f}%\")\n",
    "print(f\"   - Elite disagreements: {len(disagreements)} teams\")\n",
    "\n",
    "print(f\"\\n4. MULTI-HOP ANALYSIS\")\n",
    "print(f\"   - Max hops: {CONFIG['max_hops']}\")\n",
    "print(f\"   - Critical for cross-conference comparisons\")\n",
    "\n",
    "print(f\"\\n5. COVERAGE BY TIER (10pt handicap)\")\n",
    "for tier_type in ['simple_tier', 'network_tier']:\n",
    "    print(f\"   {tier_type.replace('_', ' ').title()}:\")\n",
    "    for tier in ['Elite', 'Mid', 'Bottom']:\n",
    "        row = df_coverage[(df_coverage['tier_type'] == tier_type) & \n",
    "                         (df_coverage['tier'] == tier) &\n",
    "                         (df_coverage['handicap'] == 10)]\n",
    "        if len(row) > 0:\n",
    "            pct = row['cover_pct'].values[0] * 100\n",
    "            print(f\"      {tier}: {pct:.1f}%\")\n",
    "\n",
    "print(f\"\\n6. NCAAM-SPECIFIC FINDINGS\")\n",
    "print(f\"   - Sparse graph requires network analysis for cross-conf comparisons\")\n",
    "print(f\"   - Community detection identifies conference-like clusters\")\n",
    "print(f\"   - Network rating helps identify overrated/underrated teams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-ratings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export team data\n",
    "export_df = df_teams[['team', 'games', 'wins', 'simple_win_pct', 'network_rating', \n",
    "                      'point_diff_avg', 'simple_sos', 'network_sos', 'community',\n",
    "                      'simple_tier', 'network_tier']].copy()\n",
    "\n",
    "export_file = Path().resolve().parent / 'data' / 'results' / 'ncaam_network_ratings.csv'\n",
    "export_df.to_csv(export_file, index=False)\n",
    "print(f\"Exported team ratings to: {export_file}\")\n",
    "\n",
    "export_df.head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}