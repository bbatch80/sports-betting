{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# NCAAM Spread Coverage Network Rating\n",
    "\n",
    "## Objective\n",
    "Build network-based strength ratings using **spread coverage results** instead of win/loss results.\n",
    "This measures which teams consistently outperform market expectations.\n",
    "\n",
    "## Key Difference from Win-Based Rating\n",
    "- **Win-based**: \"Team A beat Team B by 14 points\"\n",
    "- **Spread-based**: \"Team A covered by 5 points against a -9 spread\"\n",
    "\n",
    "## NCAAM-Specific Characteristics\n",
    "- 350+ teams, sparse graph (most teams don't play each other)\n",
    "- Strong conference clustering\n",
    "- `margin_cap=12` (~12 pts max impact)\n",
    "- `recency_decay=0.85` (faster decay, more variability)\n",
    "- `learning_rate=0.03` (smaller due to sparse connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# NCAAM Spread Coverage Config\n",
    "CONFIG = {\n",
    "    'sport': 'NCAAM',\n",
    "    'margin_cap': 12,        # ~12 pts max impact\n",
    "    'recency_decay': 0.85,   # Faster decay, more variability\n",
    "    'learning_rate': 0.03,   # Smaller due to sparse connections\n",
    "    'iterations': 150,       # More iterations for sparse graph\n",
    "    'tolerance': 0.0005,     # Tighter tolerance\n",
    "    'min_games': 5           # Minimum games for reliable rating\n",
    "}\n",
    "\n",
    "print(f\"Spread Coverage Config: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## Phase 1: Data Loading & Spread Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NCAAM data\n",
    "data_file = Path().resolve().parent / 'data' / 'results' / 'ncaam_season_results.xlsx'\n",
    "df = pd.read_excel(data_file)\n",
    "\n",
    "print(f\"Loaded {len(df)} NCAAM games\")\n",
    "print(f\"Date range: {df['game_date'].min().date()} to {df['game_date'].max().date()}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "\n",
    "# Validate spread data availability\n",
    "print(f\"\\nSpread Data Quality:\")\n",
    "print(f\"  spread_result_difference available: {df['spread_result_difference'].notna().sum()} / {len(df)} games\")\n",
    "print(f\"  Missing spread data: {df['spread_result_difference'].isna().sum()} games\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prep-spread-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare spread coverage data\n",
    "# spread_result_difference: positive = home covered, negative = away covered\n",
    "\n",
    "# Filter to games with spread data\n",
    "df_spread = df[df['spread_result_difference'].notna()].copy()\n",
    "print(f\"Games with spread data: {len(df_spread)}\")\n",
    "\n",
    "# Determine cover team and fail team\n",
    "df_spread['cover_team'] = np.where(\n",
    "    df_spread['spread_result_difference'] >= 0,\n",
    "    df_spread['home_team'],\n",
    "    df_spread['away_team']\n",
    ")\n",
    "df_spread['fail_team'] = np.where(\n",
    "    df_spread['spread_result_difference'] >= 0,\n",
    "    df_spread['away_team'],\n",
    "    df_spread['home_team']\n",
    ")\n",
    "df_spread['cover_margin'] = df_spread['spread_result_difference'].abs()\n",
    "\n",
    "# Get all teams\n",
    "all_teams = set(df_spread['home_team'].unique()) | set(df_spread['away_team'].unique())\n",
    "print(f\"Total teams: {len(all_teams)}\")\n",
    "\n",
    "# Count games per team\n",
    "team_game_counts = {}\n",
    "for team in all_teams:\n",
    "    count = len(df_spread[(df_spread['home_team'] == team) | (df_spread['away_team'] == team)])\n",
    "    team_game_counts[team] = count\n",
    "\n",
    "# Filter to teams with minimum games\n",
    "active_teams = {t for t, c in team_game_counts.items() if c >= CONFIG['min_games']}\n",
    "print(f\"Teams with {CONFIG['min_games']}+ games: {len(active_teams)}\")\n",
    "\n",
    "# Filter games to only include active teams\n",
    "df_spread_filtered = df_spread[\n",
    "    (df_spread['home_team'].isin(active_teams)) & \n",
    "    (df_spread['away_team'].isin(active_teams))\n",
    "].copy()\n",
    "print(f\"Games between active teams: {len(df_spread_filtered)}\")\n",
    "\n",
    "# Distribution of cover margins\n",
    "print(f\"\\nCover Margin Stats:\")\n",
    "print(df_spread_filtered['cover_margin'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spread coverage distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Cover margin distribution\n",
    "axes[0].hist(df_spread_filtered['cover_margin'], bins=40, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=CONFIG['margin_cap'], color='r', linestyle='--', label=f\"Margin Cap ({CONFIG['margin_cap']}pts)\")\n",
    "axes[0].set_xlabel('Cover Margin (points)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Spread Cover Margins')\n",
    "axes[0].legend()\n",
    "\n",
    "# Home vs Away cover rate\n",
    "home_covers = (df_spread_filtered['spread_result_difference'] >= 0).mean() * 100\n",
    "away_covers = (df_spread_filtered['spread_result_difference'] < 0).mean() * 100\n",
    "axes[1].bar(['Home Covers', 'Away Covers'], [home_covers, away_covers], color=['#3498db', '#e74c3c'])\n",
    "axes[1].axhline(y=50, color='gray', linestyle='--')\n",
    "axes[1].set_ylabel('Percentage')\n",
    "axes[1].set_title(f'Home vs Away Cover Rate\\n(Home: {home_covers:.1f}%, Away: {away_covers:.1f}%)')\n",
    "\n",
    "# Games per team distribution\n",
    "game_counts = list(team_game_counts.values())\n",
    "axes[2].hist(game_counts, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[2].axvline(x=CONFIG['min_games'], color='r', linestyle='--', label=f\"Min Games ({CONFIG['min_games']})\")\n",
    "axes[2].set_xlabel('Games per Team')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Distribution of Games per Team')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-header",
   "metadata": {},
   "source": [
    "## Phase 2: Build Spread Coverage Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-coverage-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_spread_coverage_network(games_df, teams, recency_decay=0.85):\n",
    "    \"\"\"\n",
    "    Build weighted directed graph from spread coverage results.\n",
    "    \n",
    "    Nodes: Teams\n",
    "    Edges: Cover_team -> Fail_team with weight = cover margin (recency-weighted)\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add all teams as nodes\n",
    "    for team in teams:\n",
    "        G.add_node(team)\n",
    "    \n",
    "    # Sort by date for recency weighting\n",
    "    games_sorted = games_df.sort_values('game_date')\n",
    "    max_date = games_sorted['game_date'].max()\n",
    "    \n",
    "    # Build edge data\n",
    "    edge_data = {}\n",
    "    \n",
    "    for _, game in games_sorted.iterrows():\n",
    "        cover_team = game['cover_team']\n",
    "        fail_team = game['fail_team']\n",
    "        cover_margin = game['cover_margin']\n",
    "        \n",
    "        # Calculate recency weight\n",
    "        days_ago = (max_date - game['game_date']).days\n",
    "        recency_weight = recency_decay ** (days_ago / 7)\n",
    "        \n",
    "        key = (cover_team, fail_team)\n",
    "        if key not in edge_data:\n",
    "            edge_data[key] = {\n",
    "                'games': 0,\n",
    "                'total_margin': 0,\n",
    "                'weighted_margin': 0,\n",
    "                'total_weight': 0\n",
    "            }\n",
    "        \n",
    "        edge_data[key]['games'] += 1\n",
    "        edge_data[key]['total_margin'] += cover_margin\n",
    "        edge_data[key]['weighted_margin'] += cover_margin * recency_weight\n",
    "        edge_data[key]['total_weight'] += recency_weight\n",
    "    \n",
    "    # Add edges to graph\n",
    "    for (cover_team, fail_team), data in edge_data.items():\n",
    "        avg_margin = data['total_margin'] / data['games']\n",
    "        weighted_avg = data['weighted_margin'] / data['total_weight']\n",
    "        \n",
    "        G.add_edge(cover_team, fail_team,\n",
    "                   games=data['games'],\n",
    "                   avg_cover_margin=avg_margin,\n",
    "                   weighted_margin=weighted_avg)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Build the spread coverage network\n",
    "G_spread = build_spread_coverage_network(df_spread_filtered, active_teams, CONFIG['recency_decay'])\n",
    "\n",
    "print(f\"Spread Coverage Network Summary:\")\n",
    "print(f\"  Nodes (teams): {G_spread.number_of_nodes()}\")\n",
    "print(f\"  Edges (cover_team -> fail_team): {G_spread.number_of_edges()}\")\n",
    "print(f\"  Avg out-degree (covers): {sum(dict(G_spread.out_degree()).values()) / G_spread.number_of_nodes():.1f}\")\n",
    "print(f\"  Density: {nx.density(G_spread):.4f}\")\n",
    "\n",
    "# Analyze connectivity\n",
    "G_undirected = G_spread.to_undirected()\n",
    "components = list(nx.connected_components(G_undirected))\n",
    "print(f\"  Connected components: {len(components)}\")\n",
    "print(f\"  Largest component size: {len(max(components, key=len))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-coverage-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the spread coverage network (subset for clarity)\n",
    "# Use the largest connected component and sample top teams\n",
    "\n",
    "# Calculate cover percentage\n",
    "cover_counts = dict(G_spread.out_degree())\n",
    "fail_counts = dict(G_spread.in_degree())\n",
    "cover_pct = {\n",
    "    team: cover_counts.get(team, 0) / (cover_counts.get(team, 0) + fail_counts.get(team, 0) + 0.001)\n",
    "    for team in active_teams\n",
    "}\n",
    "\n",
    "# Get top 50 teams by total games for visualization\n",
    "top_teams = sorted(team_game_counts.keys(), key=lambda t: team_game_counts.get(t, 0), reverse=True)[:50]\n",
    "top_teams = [t for t in top_teams if t in active_teams]\n",
    "\n",
    "# Create subgraph\n",
    "G_sub = G_spread.subgraph(top_teams)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 16))\n",
    "\n",
    "# Node colors based on cover%\n",
    "node_colors = [cover_pct.get(node, 0.5) for node in G_sub.nodes()]\n",
    "\n",
    "# Layout\n",
    "pos = nx.spring_layout(G_sub, k=3, iterations=100, seed=42)\n",
    "\n",
    "# Draw\n",
    "nodes = nx.draw_networkx_nodes(G_sub, pos, node_color=node_colors, cmap=plt.cm.RdYlGn,\n",
    "                               node_size=400, alpha=0.8, vmin=0.35, vmax=0.65, ax=ax)\n",
    "nx.draw_networkx_labels(G_sub, pos, font_size=6, ax=ax)\n",
    "nx.draw_networkx_edges(G_sub, pos, alpha=0.1, arrows=True,\n",
    "                       edge_color='gray', arrowsize=6, ax=ax)\n",
    "\n",
    "ax.set_title(f'NCAAM Spread Coverage Network (Top {len(top_teams)} Teams by Games)\\nNode color: Cover %', fontsize=14)\n",
    "fig.colorbar(nodes, ax=ax, label='Cover %')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rating-header",
   "metadata": {},
   "source": [
    "## Phase 3: Iterative Spread Coverage Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-rating-algo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spread_coverage_rating(games_df, teams, max_iterations=150, tolerance=0.0005, \n",
    "                                    margin_cap=12, learning_rate=0.03):\n",
    "    \"\"\"\n",
    "    Compute network-propagated spread coverage ratings.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Initialize all teams with rating = 0.5\n",
    "    2. For each game:\n",
    "       - Expected outcome based on current ratings\n",
    "       - Surprise factor = 1 - expected\n",
    "       - Adjustment = surprise * cover_margin (capped) * learning_rate\n",
    "       - Cover team rating += adjustment, Fail team -= adjustment\n",
    "    3. Normalize to [0, 1]\n",
    "    4. Repeat until convergence\n",
    "    \"\"\"\n",
    "    # Initialize ratings\n",
    "    ratings = {team: 0.5 for team in teams}\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        new_ratings = ratings.copy()\n",
    "        \n",
    "        for _, game in games_df.iterrows():\n",
    "            cover_team = game['cover_team']\n",
    "            fail_team = game['fail_team']\n",
    "            \n",
    "            if cover_team not in teams or fail_team not in teams:\n",
    "                continue\n",
    "                \n",
    "            cover_margin = min(game['cover_margin'], margin_cap)\n",
    "            \n",
    "            cover_rating = ratings[cover_team]\n",
    "            fail_rating = ratings[fail_team]\n",
    "            \n",
    "            # Expected outcome\n",
    "            total = cover_rating + fail_rating\n",
    "            if total == 0:\n",
    "                expected = 0.5\n",
    "            else:\n",
    "                expected = cover_rating / total\n",
    "            \n",
    "            # Surprise factor\n",
    "            surprise = 1 - expected\n",
    "            \n",
    "            # Adjustment\n",
    "            adjustment = surprise * (cover_margin / margin_cap) * learning_rate\n",
    "            \n",
    "            new_ratings[cover_team] += adjustment\n",
    "            new_ratings[fail_team] -= adjustment\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        min_r = min(new_ratings.values())\n",
    "        max_r = max(new_ratings.values())\n",
    "        if max_r > min_r:\n",
    "            new_ratings = {t: (r - min_r) / (max_r - min_r) for t, r in new_ratings.items()}\n",
    "        \n",
    "        # Check convergence\n",
    "        max_change = max(abs(new_ratings[t] - ratings[t]) for t in teams)\n",
    "        history.append(max_change)\n",
    "        \n",
    "        if max_change < tolerance:\n",
    "            print(f\"Converged at iteration {iteration + 1}\")\n",
    "            break\n",
    "        \n",
    "        ratings = new_ratings\n",
    "    \n",
    "    return ratings, history\n",
    "\n",
    "# Compute spread coverage ratings\n",
    "spread_ratings, convergence_history = compute_spread_coverage_rating(\n",
    "    df_spread_filtered,\n",
    "    active_teams,\n",
    "    max_iterations=CONFIG['iterations'],\n",
    "    tolerance=CONFIG['tolerance'],\n",
    "    margin_cap=CONFIG['margin_cap'],\n",
    "    learning_rate=CONFIG['learning_rate']\n",
    ")\n",
    "\n",
    "# Display top teams by spread coverage rating\n",
    "spread_ratings_df = pd.DataFrame([\n",
    "    {'team': team, 'spread_coverage_rating': rating, 'games': team_game_counts.get(team, 0)}\n",
    "    for team, rating in spread_ratings.items()\n",
    "]).sort_values('spread_coverage_rating', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Teams by Spread Coverage Rating:\")\n",
    "print(spread_ratings_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(convergence_history)\n",
    "plt.axhline(y=CONFIG['tolerance'], color='r', linestyle='--', label=f\"Tolerance ({CONFIG['tolerance']})\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Max Rating Change')\n",
    "plt.title('Convergence of Spread Coverage Rating Algorithm')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-header",
   "metadata": {},
   "source": [
    "## Phase 4: Compare Spread Rating vs Win-Based Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-win-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute traditional win-based rating for comparison\n",
    "def compute_win_rating(games_df, teams, max_iterations=150, tolerance=0.0005, \n",
    "                       margin_cap=20, learning_rate=0.03):\n",
    "    \"\"\"Win-based network rating for comparison.\"\"\"\n",
    "    games = games_df.copy()\n",
    "    games['home_margin'] = games['home_score'] - games['away_score']\n",
    "    games['winner'] = np.where(games['home_margin'] > 0, games['home_team'],\n",
    "                               np.where(games['home_margin'] < 0, games['away_team'], None))\n",
    "    games['loser'] = np.where(games['home_margin'] > 0, games['away_team'],\n",
    "                              np.where(games['home_margin'] < 0, games['home_team'], None))\n",
    "    games['margin'] = games['home_margin'].abs()\n",
    "    games = games[games['winner'].notna()]\n",
    "    \n",
    "    ratings = {team: 0.5 for team in teams}\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        new_ratings = ratings.copy()\n",
    "        \n",
    "        for _, game in games.iterrows():\n",
    "            winner = game['winner']\n",
    "            loser = game['loser']\n",
    "            \n",
    "            if winner not in teams or loser not in teams:\n",
    "                continue\n",
    "                \n",
    "            margin = min(game['margin'], margin_cap)\n",
    "            \n",
    "            total = ratings[winner] + ratings[loser]\n",
    "            expected = ratings[winner] / total if total > 0 else 0.5\n",
    "            surprise = 1 - expected\n",
    "            adjustment = surprise * (margin / margin_cap) * learning_rate\n",
    "            \n",
    "            new_ratings[winner] += adjustment\n",
    "            new_ratings[loser] -= adjustment\n",
    "        \n",
    "        min_r, max_r = min(new_ratings.values()), max(new_ratings.values())\n",
    "        if max_r > min_r:\n",
    "            new_ratings = {t: (r - min_r) / (max_r - min_r) for t, r in new_ratings.items()}\n",
    "        \n",
    "        max_change = max(abs(new_ratings[t] - ratings[t]) for t in teams)\n",
    "        if max_change < tolerance:\n",
    "            break\n",
    "        ratings = new_ratings\n",
    "    \n",
    "    return ratings\n",
    "\n",
    "# Filter df to active teams\n",
    "df_active = df[\n",
    "    (df['home_team'].isin(active_teams)) & \n",
    "    (df['away_team'].isin(active_teams))\n",
    "].copy()\n",
    "\n",
    "# Compute win-based ratings\n",
    "win_ratings = compute_win_rating(df_active, active_teams, margin_cap=20)\n",
    "\n",
    "print(f\"Win-based ratings computed for {len(win_ratings)} teams.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison dataframe\n",
    "team_stats = []\n",
    "\n",
    "for team in active_teams:\n",
    "    # Games data\n",
    "    home_spread = df_spread_filtered[df_spread_filtered['home_team'] == team]\n",
    "    away_spread = df_spread_filtered[df_spread_filtered['away_team'] == team]\n",
    "    total_games = len(home_spread) + len(away_spread)\n",
    "    \n",
    "    # Win stats\n",
    "    home_all = df_active[df_active['home_team'] == team]\n",
    "    away_all = df_active[df_active['away_team'] == team]\n",
    "    home_wins = (home_all['home_score'] > home_all['away_score']).sum()\n",
    "    away_wins = (away_all['away_score'] > away_all['home_score']).sum()\n",
    "    total_wins = home_wins + away_wins\n",
    "    total_all = len(home_all) + len(away_all)\n",
    "    \n",
    "    # Cover stats\n",
    "    home_covers = (home_spread['spread_result_difference'] >= 0).sum()\n",
    "    away_covers = (away_spread['spread_result_difference'] < 0).sum()\n",
    "    total_covers = home_covers + away_covers\n",
    "    \n",
    "    team_stats.append({\n",
    "        'team': team,\n",
    "        'games': total_games,\n",
    "        'wins': total_wins,\n",
    "        'win_pct': total_wins / total_all if total_all > 0 else 0,\n",
    "        'covers': total_covers,\n",
    "        'cover_pct': total_covers / total_games if total_games > 0 else 0,\n",
    "        'win_rating': win_ratings.get(team, 0.5),\n",
    "        'spread_rating': spread_ratings.get(team, 0.5)\n",
    "    })\n",
    "\n",
    "df_compare = pd.DataFrame(team_stats)\n",
    "\n",
    "# Calculate Value Score\n",
    "df_compare['value_score'] = df_compare['spread_rating'] - df_compare['win_rating']\n",
    "\n",
    "df_compare = df_compare.sort_values('value_score', ascending=False)\n",
    "\n",
    "print(\"Top 20 Teams by Value Score (Spread - Win Rating):\")\n",
    "print(\"=\"*90)\n",
    "print(df_compare[['team', 'games', 'win_pct', 'cover_pct', 'win_rating', 'spread_rating', 'value_score']].head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Win Rating vs Spread Rating\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Scatter: Win Rating vs Spread Rating\n",
    "ax = axes[0]\n",
    "colors = df_compare['value_score']\n",
    "scatter = ax.scatter(df_compare['win_rating'], df_compare['spread_rating'], \n",
    "                     c=colors, cmap='RdYlGn', s=30, alpha=0.6)\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Equal ratings')\n",
    "ax.set_xlabel('Win-Based Rating')\n",
    "ax.set_ylabel('Spread Coverage Rating')\n",
    "ax.set_title('Win Rating vs Spread Rating\\n(Color = Value Score)')\n",
    "fig.colorbar(scatter, ax=ax, label='Value Score')\n",
    "\n",
    "# Histogram of Value Scores\n",
    "ax = axes[1]\n",
    "ax.hist(df_compare['value_score'], bins=40, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "ax.axvline(x=0.05, color='green', linestyle='--', label='Undervalued threshold')\n",
    "ax.axvline(x=-0.05, color='red', linestyle='--', label='Overvalued threshold')\n",
    "ax.set_xlabel('Value Score (Spread Rating - Win Rating)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Distribution of Value Scores')\n",
    "ax.legend()\n",
    "\n",
    "# Correlation scatter\n",
    "ax = axes[2]\n",
    "corr = df_compare['win_rating'].corr(df_compare['spread_rating'])\n",
    "ax.scatter(df_compare['cover_pct'], df_compare['spread_rating'], alpha=0.5, s=20)\n",
    "ax.set_xlabel('Actual Cover %')\n",
    "ax.set_ylabel('Spread Coverage Rating')\n",
    "ax.set_title(f'Spread Rating vs Actual Cover %\\n(Correlation: {df_compare[\"cover_pct\"].corr(df_compare[\"spread_rating\"]):.3f})')\n",
    "z = np.polyfit(df_compare['cover_pct'], df_compare['spread_rating'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(sorted(df_compare['cover_pct']), p(sorted(df_compare['cover_pct'])), 'r--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCorrelation between Win Rating and Spread Rating: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undervalued-header",
   "metadata": {},
   "source": [
    "## Phase 5: Identify Undervalued & Overvalued Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identify-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds for undervalued/overvalued\n",
    "value_threshold = 0.05\n",
    "min_games_for_value = 10  # More games needed for reliable value assessment\n",
    "\n",
    "# Filter to teams with sufficient games\n",
    "df_compare_reliable = df_compare[df_compare['games'] >= min_games_for_value]\n",
    "\n",
    "undervalued = df_compare_reliable[df_compare_reliable['value_score'] > value_threshold].sort_values('value_score', ascending=False)\n",
    "overvalued = df_compare_reliable[df_compare_reliable['value_score'] < -value_threshold].sort_values('value_score')\n",
    "fair_value = df_compare_reliable[(df_compare_reliable['value_score'] >= -value_threshold) & \n",
    "                                  (df_compare_reliable['value_score'] <= value_threshold)]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"MARKET INEFFICIENCY ANALYSIS (Teams with {min_games_for_value}+ games)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸŸ¢ UNDERVALUED TEAMS ({len(undervalued)} teams)\")\n",
    "print(\"These teams cover spreads more often than their win record suggests.\")\n",
    "print(\"-\"*80)\n",
    "if len(undervalued) > 0:\n",
    "    print(undervalued[['team', 'games', 'win_pct', 'cover_pct', 'win_rating', 'spread_rating', 'value_score']].head(15).to_string(index=False))\n",
    "else:\n",
    "    print(\"No significantly undervalued teams found.\")\n",
    "\n",
    "print(f\"\\nðŸ”´ OVERVALUED TEAMS ({len(overvalued)} teams)\")\n",
    "print(\"These teams fail to cover spreads despite their win record.\")\n",
    "print(\"-\"*80)\n",
    "if len(overvalued) > 0:\n",
    "    print(overvalued[['team', 'games', 'win_pct', 'cover_pct', 'win_rating', 'spread_rating', 'value_score']].head(15).to_string(index=False))\n",
    "else:\n",
    "    print(\"No significantly overvalued teams found.\")\n",
    "\n",
    "print(f\"\\nâšª FAIR VALUE TEAMS ({len(fair_value)} teams)\")\n",
    "print(\"These teams cover spreads roughly in line with expectations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictive-header",
   "metadata": {},
   "source": [
    "## Phase 6: Predictive Analysis - Future Coverage Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing periods\n",
    "df_spread_sorted = df_spread_filtered.sort_values('game_date')\n",
    "split_idx = int(len(df_spread_sorted) * 0.75)\n",
    "\n",
    "train_df = df_spread_sorted.iloc[:split_idx]\n",
    "test_df = df_spread_sorted.iloc[split_idx:]\n",
    "\n",
    "print(f\"Training set: {len(train_df)} games ({train_df['game_date'].min().date()} to {train_df['game_date'].max().date()})\")\n",
    "print(f\"Test set: {len(test_df)} games ({test_df['game_date'].min().date()} to {test_df['game_date'].max().date()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spread rating on training data\n",
    "train_teams = set(train_df['home_team']) | set(train_df['away_team'])\n",
    "train_teams = train_teams.intersection(active_teams)\n",
    "\n",
    "train_ratings, _ = compute_spread_coverage_rating(\n",
    "    train_df,\n",
    "    train_teams,\n",
    "    max_iterations=CONFIG['iterations'],\n",
    "    tolerance=CONFIG['tolerance'],\n",
    "    margin_cap=CONFIG['margin_cap'],\n",
    "    learning_rate=CONFIG['learning_rate']\n",
    ")\n",
    "\n",
    "# Calculate actual cover rate in test period\n",
    "test_cover_rates = {}\n",
    "test_game_counts = {}\n",
    "\n",
    "for team in train_teams:\n",
    "    home_test = test_df[test_df['home_team'] == team]\n",
    "    away_test = test_df[test_df['away_team'] == team]\n",
    "    \n",
    "    total_test = len(home_test) + len(away_test)\n",
    "    if total_test >= 3:  # Require at least 3 test games\n",
    "        covers = (home_test['spread_result_difference'] >= 0).sum() + (away_test['spread_result_difference'] < 0).sum()\n",
    "        test_cover_rates[team] = covers / total_test\n",
    "        test_game_counts[team] = total_test\n",
    "\n",
    "# Compare training spread rating to test cover rate\n",
    "predictive_df = pd.DataFrame([\n",
    "    {\n",
    "        'team': team,\n",
    "        'train_spread_rating': train_ratings.get(team, 0.5),\n",
    "        'test_cover_rate': test_cover_rates.get(team, np.nan),\n",
    "        'test_games': test_game_counts.get(team, 0)\n",
    "    }\n",
    "    for team in train_teams if team in test_cover_rates\n",
    "])\n",
    "\n",
    "# Correlation\n",
    "pred_corr = predictive_df['train_spread_rating'].corr(predictive_df['test_cover_rate'])\n",
    "\n",
    "print(f\"\\nPredictive Analysis:\")\n",
    "print(f\"Teams with test data: {len(predictive_df)}\")\n",
    "print(f\"Correlation between Training Spread Rating and Test Cover Rate: {pred_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-predictive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictive power\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter: Training Rating vs Test Cover Rate\n",
    "ax = axes[0]\n",
    "ax.scatter(predictive_df['train_spread_rating'], predictive_df['test_cover_rate'], \n",
    "           alpha=0.5, s=30)\n",
    "ax.set_xlabel('Training Spread Rating')\n",
    "ax.set_ylabel('Test Period Cover Rate')\n",
    "ax.set_title(f'Predictive Power: Training Rating vs Future Coverage\\nCorrelation: {pred_corr:.3f}')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(predictive_df['train_spread_rating'], predictive_df['test_cover_rate'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(predictive_df['train_spread_rating'].min(), predictive_df['train_spread_rating'].max(), 100)\n",
    "ax.plot(x_line, p(x_line), 'r--', label=f'Trend (slope={z[0]:.2f})')\n",
    "ax.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.legend()\n",
    "\n",
    "# Tier-based analysis\n",
    "ax = axes[1]\n",
    "try:\n",
    "    predictive_df['tier'] = pd.qcut(predictive_df['train_spread_rating'], q=4, labels=['Q1 (Low)', 'Q2', 'Q3', 'Q4 (High)'])\n",
    "    tier_cover = predictive_df.groupby('tier')['test_cover_rate'].mean()\n",
    "\n",
    "    colors = ['#e74c3c', '#f39c12', '#3498db', '#2ecc71']\n",
    "    bars = ax.bar(tier_cover.index, tier_cover.values * 100, color=colors)\n",
    "    ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5, label='50% (breakeven)')\n",
    "    ax.set_xlabel('Training Spread Rating Quartile')\n",
    "    ax.set_ylabel('Test Period Cover Rate (%)')\n",
    "    ax.set_title('Future Cover Rate by Training Rating Quartile')\n",
    "    ax.legend()\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, tier_cover.values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{val*100:.1f}%', \n",
    "                ha='center', fontsize=9)\n",
    "except Exception as e:\n",
    "    ax.text(0.5, 0.5, f\"Could not create quartiles\\n{str(e)}\", ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conference-header",
   "metadata": {},
   "source": [
    "## Phase 7: Conference-Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conference-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze spread ratings by conference (if available)\n",
    "# For NCAAM, identify clusters of teams that might represent conferences\n",
    "\n",
    "# Use community detection to identify clusters\n",
    "try:\n",
    "    from networkx.algorithms import community\n",
    "    \n",
    "    # Convert to undirected for community detection\n",
    "    G_undirected = G_spread.to_undirected()\n",
    "    communities = list(community.louvain_communities(G_undirected, seed=42))\n",
    "    \n",
    "    print(f\"Detected {len(communities)} communities (potential conference-like clusters)\")\n",
    "    \n",
    "    # Assign community to each team\n",
    "    team_community = {}\n",
    "    for i, comm in enumerate(communities):\n",
    "        for team in comm:\n",
    "            team_community[team] = i\n",
    "    \n",
    "    # Add community to comparison df\n",
    "    df_compare['community'] = df_compare['team'].map(team_community)\n",
    "    \n",
    "    # Community-level stats\n",
    "    comm_stats = df_compare.groupby('community').agg({\n",
    "        'team': 'count',\n",
    "        'spread_rating': 'mean',\n",
    "        'win_rating': 'mean',\n",
    "        'value_score': 'mean',\n",
    "        'cover_pct': 'mean'\n",
    "    }).rename(columns={'team': 'teams'})\n",
    "    \n",
    "    # Filter to meaningful communities\n",
    "    comm_stats = comm_stats[comm_stats['teams'] >= 5].sort_values('value_score', ascending=False)\n",
    "    \n",
    "    print(f\"\\nCommunity-Level Analysis (communities with 5+ teams):\")\n",
    "    print(\"=\"*80)\n",
    "    print(comm_stats.round(3).to_string())\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Community detection requires networkx >= 2.8. Skipping conference analysis.\")\n",
    "except Exception as e:\n",
    "    print(f\"Community detection failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"NCAAM SPREAD COVERAGE NETWORK ANALYSIS: SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. DATA\")\n",
    "print(f\"   - Total games with spread data: {len(df_spread)}\")\n",
    "print(f\"   - Games between active teams: {len(df_spread_filtered)}\")\n",
    "print(f\"   - Active teams ({CONFIG['min_games']}+ games): {len(active_teams)}\")\n",
    "print(f\"   - Network edges: {G_spread.number_of_edges()}\")\n",
    "print(f\"   - Network density: {nx.density(G_spread):.4f}\")\n",
    "\n",
    "win_spread_corr = df_compare['win_rating'].corr(df_compare['spread_rating'])\n",
    "cover_spread_corr = df_compare['cover_pct'].corr(df_compare['spread_rating'])\n",
    "\n",
    "print(f\"\\n2. RATING VALIDATION\")\n",
    "print(f\"   - Win Rating vs Spread Rating correlation: {win_spread_corr:.3f}\")\n",
    "print(f\"   - Actual Cover % vs Spread Rating correlation: {cover_spread_corr:.3f}\")\n",
    "\n",
    "print(f\"\\n3. MARKET INEFFICIENCIES (teams with {min_games_for_value}+ games)\")\n",
    "print(f\"   - Undervalued teams: {len(undervalued)}\")\n",
    "print(f\"   - Overvalued teams: {len(overvalued)}\")\n",
    "print(f\"   - Fair value teams: {len(fair_value)}\")\n",
    "\n",
    "if len(undervalued) > 0:\n",
    "    top_undervalued = undervalued.iloc[0]\n",
    "    print(f\"   - Most undervalued: {top_undervalued['team']} (value score: +{top_undervalued['value_score']:.3f})\")\n",
    "if len(overvalued) > 0:\n",
    "    top_overvalued = overvalued.iloc[0]\n",
    "    print(f\"   - Most overvalued: {top_overvalued['team']} (value score: {top_overvalued['value_score']:.3f})\")\n",
    "\n",
    "print(f\"\\n4. PREDICTIVE POWER\")\n",
    "print(f\"   - Teams with test data: {len(predictive_df)}\")\n",
    "print(f\"   - Training-to-Test correlation: {pred_corr:.3f}\")\n",
    "if pred_corr > 0.3:\n",
    "    print(f\"   - Strong predictive signal detected\")\n",
    "elif pred_corr > 0.1:\n",
    "    print(f\"   - Moderate predictive signal detected\")\n",
    "else:\n",
    "    print(f\"   - Weak predictive signal - may be noise in sparse network\")\n",
    "\n",
    "print(f\"\\n5. NCAAM-SPECIFIC CONSIDERATIONS\")\n",
    "print(f\"   - Sparse network (low density): requires careful interpretation\")\n",
    "print(f\"   - Conference clustering affects common opponent paths\")\n",
    "print(f\"   - Higher variance due to fewer games per team\")\n",
    "\n",
    "print(f\"\\n6. RECOMMENDATIONS\")\n",
    "if win_spread_corr < 0.8:\n",
    "    print(f\"   âœ“ Spread rating captures different signal than win rating\")\n",
    "    print(f\"   âœ“ Value score may identify market inefficiencies\")\n",
    "else:\n",
    "    print(f\"   âš  Spread rating highly correlated with win rating\")\n",
    "    print(f\"   âš  May not add significant value in NCAAM\")\n",
    "\n",
    "if pred_corr > 0.15:\n",
    "    print(f\"   âœ“ Consider using spread rating for future predictions\")\n",
    "else:\n",
    "    print(f\"   âš  Combine spread rating with other features for predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-ratings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export team data with all metrics\n",
    "export_df = df_compare[['team', 'games', 'wins', 'win_pct', 'covers', 'cover_pct',\n",
    "                        'win_rating', 'spread_rating', 'value_score']].copy()\n",
    "export_df = export_df.sort_values('spread_rating', ascending=False)\n",
    "\n",
    "export_file = Path().resolve().parent / 'data' / 'results' / 'ncaam_spread_coverage_ratings.csv'\n",
    "export_df.to_csv(export_file, index=False)\n",
    "print(f\"Exported spread coverage ratings to: {export_file}\")\n",
    "\n",
    "print(f\"\\nTop 15 Teams by Spread Coverage Rating:\")\n",
    "export_df.head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
