{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# NFL Spread Coverage Network Rating\n",
    "\n",
    "## Objective\n",
    "Build network-based strength ratings using **spread coverage results** instead of win/loss results.\n",
    "This measures which teams consistently outperform market expectations.\n",
    "\n",
    "## Key Difference from Win-Based Rating\n",
    "- **Win-based**: \"Team A beat Team B by 14 points\"\n",
    "- **Spread-based**: \"Team A covered by 7 points against a -7 spread\"\n",
    "\n",
    "## NFL-Specific Parameters\n",
    "- 32 teams, 17-game regular season\n",
    "- `margin_cap=14` (2 TDs max impact on spread coverage)\n",
    "- `recency_decay=0.9`\n",
    "- `learning_rate=0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# NFL Spread Coverage Config\n",
    "CONFIG = {\n",
    "    'sport': 'NFL',\n",
    "    'margin_cap': 14,        # 2 TDs max impact on spread\n",
    "    'recency_decay': 0.9,    # Recent games important\n",
    "    'learning_rate': 0.1,    # Rating adjustment rate\n",
    "    'iterations': 100,\n",
    "    'tolerance': 0.001\n",
    "}\n",
    "\n",
    "print(f\"Spread Coverage Config: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-header",
   "metadata": {},
   "source": [
    "## Phase 1: Data Loading & Spread Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NFL data\n",
    "data_file = Path().resolve().parent / 'data' / 'results' / 'nfl_season_results.xlsx'\n",
    "df = pd.read_excel(data_file)\n",
    "\n",
    "print(f\"Loaded {len(df)} NFL games\")\n",
    "print(f\"Date range: {df['game_date'].min().date()} to {df['game_date'].max().date()}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "\n",
    "# Validate spread data availability\n",
    "print(f\"\\nSpread Data Quality:\")\n",
    "print(f\"  spread_result_difference available: {df['spread_result_difference'].notna().sum()} / {len(df)} games\")\n",
    "print(f\"  Missing spread data: {df['spread_result_difference'].isna().sum()} games\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prep-spread-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare spread coverage data\n",
    "# spread_result_difference: positive = home covered, negative = away covered\n",
    "\n",
    "# Filter to games with spread data\n",
    "df_spread = df[df['spread_result_difference'].notna()].copy()\n",
    "print(f\"Games with spread data: {len(df_spread)}\")\n",
    "\n",
    "# Determine cover team and fail team\n",
    "df_spread['cover_team'] = np.where(\n",
    "    df_spread['spread_result_difference'] >= 0,\n",
    "    df_spread['home_team'],\n",
    "    df_spread['away_team']\n",
    ")\n",
    "df_spread['fail_team'] = np.where(\n",
    "    df_spread['spread_result_difference'] >= 0,\n",
    "    df_spread['away_team'],\n",
    "    df_spread['home_team']\n",
    ")\n",
    "df_spread['cover_margin'] = df_spread['spread_result_difference'].abs()\n",
    "\n",
    "# Get all teams\n",
    "all_teams = set(df_spread['home_team'].unique()) | set(df_spread['away_team'].unique())\n",
    "print(f\"Total teams: {len(all_teams)}\")\n",
    "\n",
    "# Distribution of cover margins\n",
    "print(f\"\\nCover Margin Stats:\")\n",
    "print(df_spread['cover_margin'].describe())\n",
    "\n",
    "df_spread[['game_date', 'home_team', 'away_team', 'spread_result_difference', 'cover_team', 'fail_team', 'cover_margin']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spread coverage distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Cover margin distribution\n",
    "axes[0].hist(df_spread['cover_margin'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=CONFIG['margin_cap'], color='r', linestyle='--', label=f\"Margin Cap ({CONFIG['margin_cap']}pts)\")\n",
    "axes[0].set_xlabel('Cover Margin (points)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Spread Cover Margins')\n",
    "axes[0].legend()\n",
    "\n",
    "# Home vs Away cover rate\n",
    "home_covers = (df_spread['spread_result_difference'] >= 0).mean() * 100\n",
    "away_covers = (df_spread['spread_result_difference'] < 0).mean() * 100\n",
    "axes[1].bar(['Home Covers', 'Away Covers'], [home_covers, away_covers], color=['#3498db', '#e74c3c'])\n",
    "axes[1].axhline(y=50, color='gray', linestyle='--')\n",
    "axes[1].set_ylabel('Percentage')\n",
    "axes[1].set_title(f'Home vs Away Cover Rate\\n(Home: {home_covers:.1f}%, Away: {away_covers:.1f}%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-header",
   "metadata": {},
   "source": [
    "## Phase 2: Build Spread Coverage Network Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-coverage-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_spread_coverage_network(games_df, recency_decay=0.9):\n",
    "    \"\"\"\n",
    "    Build weighted directed graph from spread coverage results.\n",
    "    \n",
    "    Nodes: Teams\n",
    "    Edges: Cover_team -> Fail_team with weight = cover margin (recency-weighted)\n",
    "    \"\"\"\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add all teams as nodes\n",
    "    for team in all_teams:\n",
    "        G.add_node(team)\n",
    "    \n",
    "    # Sort by date for recency weighting\n",
    "    games_sorted = games_df.sort_values('game_date')\n",
    "    max_date = games_sorted['game_date'].max()\n",
    "    \n",
    "    # Build edge data\n",
    "    edge_data = {}\n",
    "    \n",
    "    for _, game in games_sorted.iterrows():\n",
    "        cover_team = game['cover_team']\n",
    "        fail_team = game['fail_team']\n",
    "        cover_margin = game['cover_margin']\n",
    "        \n",
    "        # Calculate recency weight\n",
    "        days_ago = (max_date - game['game_date']).days\n",
    "        recency_weight = recency_decay ** (days_ago / 7)\n",
    "        \n",
    "        key = (cover_team, fail_team)\n",
    "        if key not in edge_data:\n",
    "            edge_data[key] = {\n",
    "                'games': 0,\n",
    "                'total_margin': 0,\n",
    "                'weighted_margin': 0,\n",
    "                'total_weight': 0\n",
    "            }\n",
    "        \n",
    "        edge_data[key]['games'] += 1\n",
    "        edge_data[key]['total_margin'] += cover_margin\n",
    "        edge_data[key]['weighted_margin'] += cover_margin * recency_weight\n",
    "        edge_data[key]['total_weight'] += recency_weight\n",
    "    \n",
    "    # Add edges to graph\n",
    "    for (cover_team, fail_team), data in edge_data.items():\n",
    "        avg_margin = data['total_margin'] / data['games']\n",
    "        weighted_avg = data['weighted_margin'] / data['total_weight']\n",
    "        \n",
    "        G.add_edge(cover_team, fail_team,\n",
    "                   games=data['games'],\n",
    "                   avg_cover_margin=avg_margin,\n",
    "                   weighted_margin=weighted_avg)\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Build the spread coverage network\n",
    "G_spread = build_spread_coverage_network(df_spread, CONFIG['recency_decay'])\n",
    "\n",
    "print(f\"Spread Coverage Network Summary:\")\n",
    "print(f\"  Nodes (teams): {G_spread.number_of_nodes()}\")\n",
    "print(f\"  Edges (cover_team -> fail_team): {G_spread.number_of_edges()}\")\n",
    "print(f\"  Avg out-degree (covers): {sum(dict(G_spread.out_degree()).values()) / G_spread.number_of_nodes():.1f}\")\n",
    "print(f\"  Density: {nx.density(G_spread):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-coverage-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the spread coverage network\n",
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "\n",
    "# Calculate cover percentage for coloring\n",
    "cover_counts = dict(G_spread.out_degree())\n",
    "fail_counts = dict(G_spread.in_degree())\n",
    "cover_pct = {\n",
    "    team: cover_counts.get(team, 0) / (cover_counts.get(team, 0) + fail_counts.get(team, 0) + 0.001)\n",
    "    for team in all_teams\n",
    "}\n",
    "\n",
    "# Node colors based on cover%\n",
    "node_colors = [cover_pct.get(node, 0.5) for node in G_spread.nodes()]\n",
    "\n",
    "# Layout\n",
    "pos = nx.spring_layout(G_spread, k=2, iterations=50, seed=42)\n",
    "\n",
    "# Draw\n",
    "nodes = nx.draw_networkx_nodes(G_spread, pos, node_color=node_colors, cmap=plt.cm.RdYlGn,\n",
    "                               node_size=500, alpha=0.8, vmin=0.3, vmax=0.7, ax=ax)\n",
    "nx.draw_networkx_labels(G_spread, pos, font_size=7, ax=ax)\n",
    "nx.draw_networkx_edges(G_spread, pos, alpha=0.15, arrows=True,\n",
    "                       edge_color='gray', arrowsize=10, ax=ax)\n",
    "\n",
    "ax.set_title('NFL Spread Coverage Network (Edge: Cover Team -> Fail Team)\\nNode color: Cover %', fontsize=14)\n",
    "fig.colorbar(nodes, ax=ax, label='Cover %')\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rating-header",
   "metadata": {},
   "source": [
    "## Phase 3: Iterative Spread Coverage Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-rating-algo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spread_coverage_rating(games_df, max_iterations=100, tolerance=0.001, \n",
    "                                    margin_cap=14, learning_rate=0.1):\n",
    "    \"\"\"\n",
    "    Compute network-propagated spread coverage ratings.\n",
    "    \n",
    "    Algorithm:\n",
    "    1. Initialize all teams with rating = 0.5\n",
    "    2. For each game:\n",
    "       - Expected outcome based on current ratings\n",
    "       - Surprise factor = 1 - expected\n",
    "       - Adjustment = surprise * cover_margin (capped) * learning_rate\n",
    "       - Cover team rating += adjustment, Fail team -= adjustment\n",
    "    3. Normalize to [0, 1]\n",
    "    4. Repeat until convergence\n",
    "    \"\"\"\n",
    "    teams = set(games_df['home_team']) | set(games_df['away_team'])\n",
    "    \n",
    "    # Initialize ratings\n",
    "    ratings = {team: 0.5 for team in teams}\n",
    "    \n",
    "    history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        new_ratings = ratings.copy()\n",
    "        \n",
    "        for _, game in games_df.iterrows():\n",
    "            cover_team = game['cover_team']\n",
    "            fail_team = game['fail_team']\n",
    "            cover_margin = min(game['cover_margin'], margin_cap)\n",
    "            \n",
    "            cover_rating = ratings[cover_team]\n",
    "            fail_rating = ratings[fail_team]\n",
    "            \n",
    "            # Expected outcome (cover team should have higher rating if good at covering)\n",
    "            total = cover_rating + fail_rating\n",
    "            if total == 0:\n",
    "                expected = 0.5\n",
    "            else:\n",
    "                expected = cover_rating / total\n",
    "            \n",
    "            # Surprise factor (unexpected cover = larger surprise)\n",
    "            surprise = 1 - expected\n",
    "            \n",
    "            # Adjustment scaled by margin\n",
    "            adjustment = surprise * (cover_margin / margin_cap) * learning_rate\n",
    "            \n",
    "            new_ratings[cover_team] += adjustment\n",
    "            new_ratings[fail_team] -= adjustment\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        min_r = min(new_ratings.values())\n",
    "        max_r = max(new_ratings.values())\n",
    "        if max_r > min_r:\n",
    "            new_ratings = {t: (r - min_r) / (max_r - min_r) for t, r in new_ratings.items()}\n",
    "        \n",
    "        # Check convergence\n",
    "        max_change = max(abs(new_ratings[t] - ratings[t]) for t in teams)\n",
    "        history.append(max_change)\n",
    "        \n",
    "        if max_change < tolerance:\n",
    "            print(f\"Converged at iteration {iteration + 1}\")\n",
    "            break\n",
    "        \n",
    "        ratings = new_ratings\n",
    "    \n",
    "    return ratings, history\n",
    "\n",
    "# Compute spread coverage ratings\n",
    "spread_ratings, convergence_history = compute_spread_coverage_rating(\n",
    "    df_spread,\n",
    "    max_iterations=CONFIG['iterations'],\n",
    "    tolerance=CONFIG['tolerance'],\n",
    "    margin_cap=CONFIG['margin_cap'],\n",
    "    learning_rate=CONFIG['learning_rate']\n",
    ")\n",
    "\n",
    "# Display top teams by spread coverage rating\n",
    "spread_ratings_df = pd.DataFrame([\n",
    "    {'team': team, 'spread_coverage_rating': rating}\n",
    "    for team, rating in spread_ratings.items()\n",
    "]).sort_values('spread_coverage_rating', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Teams by Spread Coverage Rating:\")\n",
    "print(spread_ratings_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-convergence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot convergence\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(convergence_history)\n",
    "plt.axhline(y=CONFIG['tolerance'], color='r', linestyle='--', label=f\"Tolerance ({CONFIG['tolerance']})\")\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Max Rating Change')\n",
    "plt.title('Convergence of Spread Coverage Rating Algorithm')\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-header",
   "metadata": {},
   "source": [
    "## Phase 4: Compare Spread Rating vs Win-Based Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compute-win-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute traditional win-based rating for comparison\n",
    "def compute_win_rating(games_df, max_iterations=100, tolerance=0.001, margin_cap=21, learning_rate=0.1):\n",
    "    \"\"\"Win-based network rating for comparison.\"\"\"\n",
    "    # Prepare win/loss data\n",
    "    games = games_df.copy()\n",
    "    games['home_margin'] = games['home_score'] - games['away_score']\n",
    "    games['winner'] = np.where(games['home_margin'] > 0, games['home_team'],\n",
    "                               np.where(games['home_margin'] < 0, games['away_team'], None))\n",
    "    games['loser'] = np.where(games['home_margin'] > 0, games['away_team'],\n",
    "                              np.where(games['home_margin'] < 0, games['home_team'], None))\n",
    "    games['margin'] = games['home_margin'].abs()\n",
    "    games = games[games['winner'].notna()]\n",
    "    \n",
    "    teams = set(games['home_team']) | set(games['away_team'])\n",
    "    ratings = {team: 0.5 for team in teams}\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        new_ratings = ratings.copy()\n",
    "        \n",
    "        for _, game in games.iterrows():\n",
    "            winner = game['winner']\n",
    "            loser = game['loser']\n",
    "            margin = min(game['margin'], margin_cap)\n",
    "            \n",
    "            total = ratings[winner] + ratings[loser]\n",
    "            expected = ratings[winner] / total if total > 0 else 0.5\n",
    "            surprise = 1 - expected\n",
    "            adjustment = surprise * (margin / margin_cap) * learning_rate\n",
    "            \n",
    "            new_ratings[winner] += adjustment\n",
    "            new_ratings[loser] -= adjustment\n",
    "        \n",
    "        min_r, max_r = min(new_ratings.values()), max(new_ratings.values())\n",
    "        if max_r > min_r:\n",
    "            new_ratings = {t: (r - min_r) / (max_r - min_r) for t, r in new_ratings.items()}\n",
    "        \n",
    "        max_change = max(abs(new_ratings[t] - ratings[t]) for t in teams)\n",
    "        if max_change < tolerance:\n",
    "            break\n",
    "        ratings = new_ratings\n",
    "    \n",
    "    return ratings\n",
    "\n",
    "# Compute win-based ratings\n",
    "win_ratings = compute_win_rating(df, margin_cap=21)\n",
    "\n",
    "print(\"Win-based ratings computed for comparison.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build comparison dataframe\n",
    "team_stats = []\n",
    "\n",
    "for team in all_teams:\n",
    "    # Games data\n",
    "    home_games = df_spread[df_spread['home_team'] == team]\n",
    "    away_games = df_spread[df_spread['away_team'] == team]\n",
    "    total_games = len(home_games) + len(away_games)\n",
    "    \n",
    "    # Win stats\n",
    "    home_wins = ((df[df['home_team'] == team]['home_score'] > df[df['home_team'] == team]['away_score'])).sum()\n",
    "    away_wins = ((df[df['away_team'] == team]['away_score'] > df[df['away_team'] == team]['home_score'])).sum()\n",
    "    total_wins = home_wins + away_wins\n",
    "    \n",
    "    # Cover stats\n",
    "    home_covers = (home_games['spread_result_difference'] >= 0).sum()\n",
    "    away_covers = (away_games['spread_result_difference'] < 0).sum()\n",
    "    total_covers = home_covers + away_covers\n",
    "    \n",
    "    team_stats.append({\n",
    "        'team': team,\n",
    "        'games': total_games,\n",
    "        'wins': total_wins,\n",
    "        'win_pct': total_wins / total_games if total_games > 0 else 0,\n",
    "        'covers': total_covers,\n",
    "        'cover_pct': total_covers / total_games if total_games > 0 else 0,\n",
    "        'win_rating': win_ratings.get(team, 0.5),\n",
    "        'spread_rating': spread_ratings.get(team, 0.5)\n",
    "    })\n",
    "\n",
    "df_compare = pd.DataFrame(team_stats)\n",
    "\n",
    "# Calculate Value Score: spread_rating - win_rating\n",
    "# Positive = undervalued by market (covers more than expected)\n",
    "# Negative = overvalued by market (covers less than expected)\n",
    "df_compare['value_score'] = df_compare['spread_rating'] - df_compare['win_rating']\n",
    "\n",
    "df_compare = df_compare.sort_values('value_score', ascending=False)\n",
    "\n",
    "print(\"Team Comparison: Win Rating vs Spread Rating\")\n",
    "print(\"=\"*80)\n",
    "print(df_compare[['team', 'win_pct', 'cover_pct', 'win_rating', 'spread_rating', 'value_score']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Win Rating vs Spread Rating\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Scatter: Win Rating vs Spread Rating\n",
    "ax = axes[0]\n",
    "colors = df_compare['value_score']\n",
    "scatter = ax.scatter(df_compare['win_rating'], df_compare['spread_rating'], \n",
    "                     c=colors, cmap='RdYlGn', s=100, alpha=0.7)\n",
    "ax.plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Equal ratings')\n",
    "ax.set_xlabel('Win-Based Rating')\n",
    "ax.set_ylabel('Spread Coverage Rating')\n",
    "ax.set_title('Win Rating vs Spread Rating\\n(Color = Value Score)')\n",
    "fig.colorbar(scatter, ax=ax, label='Value Score')\n",
    "\n",
    "# Annotate outliers\n",
    "for _, row in df_compare.head(3).iterrows():\n",
    "    ax.annotate(row['team'], (row['win_rating'], row['spread_rating']), fontsize=8)\n",
    "for _, row in df_compare.tail(3).iterrows():\n",
    "    ax.annotate(row['team'], (row['win_rating'], row['spread_rating']), fontsize=8)\n",
    "\n",
    "# Bar: Value Scores\n",
    "ax = axes[1]\n",
    "sorted_df = df_compare.sort_values('value_score')\n",
    "colors = ['#e74c3c' if v < 0 else '#2ecc71' for v in sorted_df['value_score']]\n",
    "ax.barh(sorted_df['team'], sorted_df['value_score'], color=colors)\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_xlabel('Value Score (Spread Rating - Win Rating)')\n",
    "ax.set_title('Value Scores by Team\\nPositive = Undervalued, Negative = Overvalued')\n",
    "\n",
    "# Correlation\n",
    "ax = axes[2]\n",
    "corr = df_compare['win_rating'].corr(df_compare['spread_rating'])\n",
    "ax.scatter(df_compare['cover_pct'], df_compare['spread_rating'], alpha=0.7)\n",
    "ax.set_xlabel('Actual Cover %')\n",
    "ax.set_ylabel('Spread Coverage Rating')\n",
    "ax.set_title(f'Spread Rating vs Actual Cover %\\n(Correlation: {df_compare[\"cover_pct\"].corr(df_compare[\"spread_rating\"]):.3f})')\n",
    "z = np.polyfit(df_compare['cover_pct'], df_compare['spread_rating'], 1)\n",
    "p = np.poly1d(z)\n",
    "ax.plot(sorted(df_compare['cover_pct']), p(sorted(df_compare['cover_pct'])), 'r--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCorrelation between Win Rating and Spread Rating: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undervalued-header",
   "metadata": {},
   "source": [
    "## Phase 5: Identify Undervalued & Overvalued Teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identify-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds for undervalued/overvalued\n",
    "value_threshold = 0.05  # 5% difference\n",
    "\n",
    "undervalued = df_compare[df_compare['value_score'] > value_threshold].sort_values('value_score', ascending=False)\n",
    "overvalued = df_compare[df_compare['value_score'] < -value_threshold].sort_values('value_score')\n",
    "fair_value = df_compare[(df_compare['value_score'] >= -value_threshold) & (df_compare['value_score'] <= value_threshold)]\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"MARKET INEFFICIENCY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nðŸŸ¢ UNDERVALUED TEAMS ({len(undervalued)} teams)\")\n",
    "print(\"These teams cover spreads more often than their win record suggests.\")\n",
    "print(\"-\"*70)\n",
    "if len(undervalued) > 0:\n",
    "    print(undervalued[['team', 'win_pct', 'cover_pct', 'win_rating', 'spread_rating', 'value_score']].to_string(index=False))\n",
    "else:\n",
    "    print(\"No significantly undervalued teams found.\")\n",
    "\n",
    "print(f\"\\nðŸ”´ OVERVALUED TEAMS ({len(overvalued)} teams)\")\n",
    "print(\"These teams fail to cover spreads despite their win record.\")\n",
    "print(\"-\"*70)\n",
    "if len(overvalued) > 0:\n",
    "    print(overvalued[['team', 'win_pct', 'cover_pct', 'win_rating', 'spread_rating', 'value_score']].to_string(index=False))\n",
    "else:\n",
    "    print(\"No significantly overvalued teams found.\")\n",
    "\n",
    "print(f\"\\nâšª FAIR VALUE TEAMS ({len(fair_value)} teams)\")\n",
    "print(\"These teams cover spreads roughly in line with expectations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "predictive-header",
   "metadata": {},
   "source": [
    "## Phase 6: Predictive Analysis - Future Coverage Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing periods\n",
    "# Use first 75% of games for training, last 25% for testing\n",
    "df_spread_sorted = df_spread.sort_values('game_date')\n",
    "split_idx = int(len(df_spread_sorted) * 0.75)\n",
    "\n",
    "train_df = df_spread_sorted.iloc[:split_idx]\n",
    "test_df = df_spread_sorted.iloc[split_idx:]\n",
    "\n",
    "print(f\"Training set: {len(train_df)} games ({train_df['game_date'].min().date()} to {train_df['game_date'].max().date()})\")\n",
    "print(f\"Test set: {len(test_df)} games ({test_df['game_date'].min().date()} to {test_df['game_date'].max().date()})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute spread rating on training data\n",
    "train_ratings, _ = compute_spread_coverage_rating(\n",
    "    train_df,\n",
    "    max_iterations=CONFIG['iterations'],\n",
    "    tolerance=CONFIG['tolerance'],\n",
    "    margin_cap=CONFIG['margin_cap'],\n",
    "    learning_rate=CONFIG['learning_rate']\n",
    ")\n",
    "\n",
    "# Calculate actual cover rate in test period\n",
    "test_cover_rates = {}\n",
    "for team in all_teams:\n",
    "    home_test = test_df[test_df['home_team'] == team]\n",
    "    away_test = test_df[test_df['away_team'] == team]\n",
    "    \n",
    "    total_test = len(home_test) + len(away_test)\n",
    "    if total_test > 0:\n",
    "        covers = (home_test['spread_result_difference'] >= 0).sum() + (away_test['spread_result_difference'] < 0).sum()\n",
    "        test_cover_rates[team] = covers / total_test\n",
    "\n",
    "# Compare training spread rating to test cover rate\n",
    "predictive_df = pd.DataFrame([\n",
    "    {\n",
    "        'team': team,\n",
    "        'train_spread_rating': train_ratings.get(team, 0.5),\n",
    "        'test_cover_rate': test_cover_rates.get(team, np.nan)\n",
    "    }\n",
    "    for team in all_teams if team in test_cover_rates\n",
    "])\n",
    "\n",
    "# Correlation\n",
    "pred_corr = predictive_df['train_spread_rating'].corr(predictive_df['test_cover_rate'])\n",
    "\n",
    "print(f\"\\nPredictive Analysis:\")\n",
    "print(f\"Correlation between Training Spread Rating and Test Cover Rate: {pred_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-predictive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictive power\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter: Training Rating vs Test Cover Rate\n",
    "ax = axes[0]\n",
    "ax.scatter(predictive_df['train_spread_rating'], predictive_df['test_cover_rate'], alpha=0.7, s=80)\n",
    "ax.set_xlabel('Training Spread Rating')\n",
    "ax.set_ylabel('Test Period Cover Rate')\n",
    "ax.set_title(f'Predictive Power: Training Rating vs Future Coverage\\nCorrelation: {pred_corr:.3f}')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(predictive_df['train_spread_rating'], predictive_df['test_cover_rate'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_line = np.linspace(predictive_df['train_spread_rating'].min(), predictive_df['train_spread_rating'].max(), 100)\n",
    "ax.plot(x_line, p(x_line), 'r--', label=f'Trend (slope={z[0]:.2f})')\n",
    "ax.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.legend()\n",
    "\n",
    "# Tier-based analysis\n",
    "ax = axes[1]\n",
    "predictive_df['tier'] = pd.qcut(predictive_df['train_spread_rating'], q=3, labels=['Bottom', 'Mid', 'Top'])\n",
    "tier_cover = predictive_df.groupby('tier')['test_cover_rate'].mean()\n",
    "\n",
    "colors = ['#e74c3c', '#f39c12', '#2ecc71']\n",
    "bars = ax.bar(tier_cover.index, tier_cover.values * 100, color=colors)\n",
    "ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5, label='50% (breakeven)')\n",
    "ax.set_xlabel('Training Spread Rating Tier')\n",
    "ax.set_ylabel('Test Period Cover Rate (%)')\n",
    "ax.set_title('Future Cover Rate by Training Rating Tier')\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, tier_cover.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{val*100:.1f}%', \n",
    "            ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTest Cover Rate by Training Tier:\")\n",
    "for tier in ['Top', 'Mid', 'Bottom']:\n",
    "    rate = tier_cover.get(tier, 0) * 100\n",
    "    print(f\"  {tier}: {rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"NFL SPREAD COVERAGE NETWORK ANALYSIS: SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. DATA\")\n",
    "print(f\"   - Games with spread data: {len(df_spread)}\")\n",
    "print(f\"   - Teams: {len(all_teams)}\")\n",
    "print(f\"   - Network edges: {G_spread.number_of_edges()}\")\n",
    "\n",
    "win_spread_corr = df_compare['win_rating'].corr(df_compare['spread_rating'])\n",
    "cover_spread_corr = df_compare['cover_pct'].corr(df_compare['spread_rating'])\n",
    "\n",
    "print(f\"\\n2. RATING VALIDATION\")\n",
    "print(f\"   - Win Rating vs Spread Rating correlation: {win_spread_corr:.3f}\")\n",
    "print(f\"   - Actual Cover % vs Spread Rating correlation: {cover_spread_corr:.3f}\")\n",
    "\n",
    "print(f\"\\n3. MARKET INEFFICIENCIES\")\n",
    "print(f\"   - Undervalued teams: {len(undervalued)}\")\n",
    "print(f\"   - Overvalued teams: {len(overvalued)}\")\n",
    "print(f\"   - Fair value teams: {len(fair_value)}\")\n",
    "\n",
    "if len(undervalued) > 0:\n",
    "    top_undervalued = undervalued.iloc[0]\n",
    "    print(f\"   - Most undervalued: {top_undervalued['team']} (value score: +{top_undervalued['value_score']:.3f})\")\n",
    "if len(overvalued) > 0:\n",
    "    top_overvalued = overvalued.iloc[0]\n",
    "    print(f\"   - Most overvalued: {top_overvalued['team']} (value score: {top_overvalued['value_score']:.3f})\")\n",
    "\n",
    "print(f\"\\n4. PREDICTIVE POWER\")\n",
    "print(f\"   - Training-to-Test correlation: {pred_corr:.3f}\")\n",
    "if pred_corr > 0.3:\n",
    "    print(f\"   - Strong predictive signal detected\")\n",
    "elif pred_corr > 0.1:\n",
    "    print(f\"   - Moderate predictive signal detected\")\n",
    "else:\n",
    "    print(f\"   - Weak predictive signal - spread rating may be noise\")\n",
    "\n",
    "print(f\"\\n5. RECOMMENDATIONS\")\n",
    "if win_spread_corr < 0.8:\n",
    "    print(f\"   âœ“ Spread rating captures different signal than win rating\")\n",
    "    print(f\"   âœ“ Value score may identify market inefficiencies\")\n",
    "else:\n",
    "    print(f\"   âš  Spread rating highly correlated with win rating\")\n",
    "    print(f\"   âš  May not add significant value over simple win-based analysis\")\n",
    "\n",
    "if pred_corr > 0.2:\n",
    "    print(f\"   âœ“ Consider using spread rating for future predictions\")\n",
    "    print(f\"   âœ“ Top-tier spread-rated teams show better future coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-ratings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export team data with all metrics\n",
    "export_df = df_compare[['team', 'games', 'wins', 'win_pct', 'covers', 'cover_pct',\n",
    "                        'win_rating', 'spread_rating', 'value_score']].copy()\n",
    "export_df = export_df.sort_values('spread_rating', ascending=False)\n",
    "\n",
    "export_file = Path().resolve().parent / 'data' / 'results' / 'nfl_spread_coverage_ratings.csv'\n",
    "export_df.to_csv(export_file, index=False)\n",
    "print(f\"Exported spread coverage ratings to: {export_file}\")\n",
    "\n",
    "export_df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
